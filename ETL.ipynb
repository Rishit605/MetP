{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray\n",
    "!pip install ecmwflibs\n",
    "!pip install cfgrib\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "txt = str(\"filter_by_keys={'typeOfLevel': 'meanSea'}\\n    filter_by_keys={'typeOfLevel': 'hybrid'}\\n    filter_by_keys={'typeOfLevel': 'atmosphere'}\\n    filter_by_keys={'typeOfLevel': 'surface'}\\n    filter_by_keys={'typeOfLevel': 'planetaryBoundaryLayer'}\\n    filter_by_keys={'typeOfLevel': 'isobaricInPa'}\\n    filter_by_keys={'typeOfLevel': 'isobaricInhPa'}\\n    filter_by_keys={'typeOfLevel': 'heightAboveGround'}\\n    filter_by_keys={'typeOfLevel': 'depthBelowLandLayer'}\\n    filter_by_keys={'typeOfLevel': 'heightAboveSea'}\\n    filter_by_keys={'typeOfLevel': 'atmosphereSingleLayer'}\\n    filter_by_keys={'typeOfLevel': 'lowCloudLayer'}\\n    filter_by_keys={'typeOfLevel': 'middleCloudLayer'}\\n    filter_by_keys={'typeOfLevel': 'highCloudLayer'}\\n    filter_by_keys={'typeOfLevel': 'cloudCeiling'}\\n    filter_by_keys={'typeOfLevel': 'heightAboveGroundLayer'}\\n    filter_by_keys={'typeOfLevel': 'tropopause'}\\n    filter_by_keys={'typeOfLevel': 'maxWind'}\\n    filter_by_keys={'typeOfLevel': 'isothermZero'}\\n    filter_by_keys={'typeOfLevel': 'highestTroposphericFreezing'}\\n    filter_by_keys={'typeOfLevel': 'pressureFromGroundLayer'}\\n    filter_by_keys={'typeOfLevel': 'sigmaLayer'}\\n    filter_by_keys={'typeOfLevel': 'sigma'}\\n    filter_by_keys={'typeOfLevel': 'potentialVorticity'}\")\n",
    "\n",
    "def cls_txt(det: str):\n",
    "\n",
    "    \n",
    "\n",
    "    det = det.lower()\n",
    "    det = det.replace(\"=\", \"\")\n",
    "    det = det.replace(\"{\", \"\")\n",
    "    det = det.replace(\"}\", \"\")\n",
    "    det = det.replace(\"\\n\", \"\")\n",
    "    det = det.replace(\":\", \"\")\n",
    "    det = det.replace(\"'\", \"\")\n",
    "    det = det.replace(\"  \", \" \")\n",
    "    det = det.replace(\"filter_by_keys\", \"\")\n",
    "    det = det.replace(\"typeoflevel\", \"\")\n",
    "    det = det.replace(\"  \", \"\")\n",
    "    det = det.lstrip()\n",
    "    det = det.rstrip()\n",
    "    det = det\n",
    "    \n",
    "    det = det.replace(\" \", \", \")\n",
    "    return det\n",
    "\n",
    "txt2 = cls_txt(txt)\n",
    "\n",
    "perams = []\n",
    "txt3 = txt2.split(',')  # Split txt2 outside the loop\n",
    "for n in range(len(txt3)):\n",
    "    txt3[n] = txt3[n].lstrip()  # Corrected: Assign back the stripped string\n",
    "    perams.append(txt3[n])  # Corrected: Append the stripped string to perams\n",
    "\n",
    "perams.remove('hybrid')\n",
    "perams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GFS DATA CURATION AND PREPROCESSING ###\n",
    "data_path = r\"C:\\Projs\\COde\\Earthquake\\earthquake-prediction\\data\\GFS\" # Change DATA PATH\n",
    "\n",
    "\n",
    "def wrf_prep_fn(pats, key_params):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(pats):\n",
    "        for f in files:\n",
    "            try:\n",
    "                if f.endswith('.grb2'):\n",
    "                    for key_param in perams:\n",
    "                        try:\n",
    "                            f_pt = os.path.join(root, f)\n",
    "                            ini_ds = open_dataset(f_pt, engine='cfgrib', backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface'}}, decode_cf=True)\n",
    "                            var_nams = list(loc_data(ini_ds, 22.71, 77.41).data_vars)\n",
    "\n",
    "                            init_df = pd.DataFrame()\n",
    "                            vals = []\n",
    "\n",
    "                            for var_name in var_nams:  # Loop through variable names\n",
    "                                \n",
    "                                data_value = loc_data(ini_ds, 22.71, 77.41)[var_name].values\n",
    "                                print(f\"{var_name}: {data_value}\")\n",
    "                                init_df[var_name] = ''  # Use variable name as column name\n",
    "\n",
    "                                vals.append(data_value)\n",
    "\n",
    "                            init_df.loc[len(init_df)] = vals\n",
    "                            ini_ds.close()\n",
    "                            return init_df\n",
    "                        except Exception as e:\n",
    "                            print(f\"{e}: Error opening the dataset file, probably hte required file format is not present.\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"{e}: Gridded Dataset File Not found, consider checking he directory.\")\n",
    "\n",
    "            # return data_value\n",
    "            \n",
    "\n",
    "dpfs = wrf_prep_fn(data_path, txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
